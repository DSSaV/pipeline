{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\35840\\Desktop\\coding\\python\\pipeline\\training.ipynb:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.processing as processing\n",
    "import ipynb.fs.full.training as training\n",
    "import ipynb.fs.full.storage as storage\n",
    "import ipynb.fs.full.misc as misc\n",
    "import ipynb.fs.full.splitting as splitting\n",
    "import ipynb.fs.full.features as features\n",
    "import ipynb.fs.full.ensemble as ensemble\n",
    "import ipynb.fs.full.profit as profit\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE REGRESSION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_dataset(config):\n",
    "    \n",
    "    # CRATE BASELINE DATAFRAME\n",
    "    dataframe = processing.create_dataframe(config)\n",
    "    \n",
    "    # ADD TYPE FEATURES\n",
    "    regression_dataset = features.add(dataframe, config['features'])\n",
    "    \n",
    "    return regression_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN & VALIDATE PIPELINE ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(config):\n",
    "    \n",
    "    # CREATE REGRESSION DATASET\n",
    "    regression_dataset = create_regression_dataset(config)\n",
    "    \n",
    "    # DROP THE CLOSE PRICE COLUMN\n",
    "    regression_dataset.drop(columns=['close'], inplace=True)\n",
    "    \n",
    "    # SPLIT INTO TRAIN & TEST DATASETS\n",
    "    primary_dataset = splitting.general(\n",
    "        regression_dataset,\n",
    "        config['splitting']['train_split']\n",
    "    )\n",
    "    \n",
    "    # TRAIN THE REGRESSION ENSEMBLE\n",
    "    regression_ensemble, regression_table = ensemble.regression(primary_dataset, config)\n",
    "    \n",
    "    # CREATE A DECISION MACHINE\n",
    "    decision_machine = misc.decision_machine()\n",
    "    \n",
    "    # PUT REGRESSION LABELS THROUGH IT\n",
    "    regression_labels = decision_machine.calibrate(\n",
    "        regression_table,\n",
    "        config['classification_ensemble']['decision']\n",
    "    )\n",
    "    \n",
    "    # REPLACE OLD LABELS WITH NEW ONES\n",
    "    labeled_regression_table = misc.replace_labels(\n",
    "        regression_table,\n",
    "        regression_labels\n",
    "    )\n",
    "    \n",
    "    # TRAIN THE CLASSIFIER ENSEMBLE\n",
    "    classifier_ensemble, classifier_table = ensemble.classifier(\n",
    "        labeled_regression_table,\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # CREATE A CONFUSION MATRIX FOR TRAIN PREDICTIONS\n",
    "    classifier_matrixes = misc.train_matrixes(\n",
    "        labeled_regression_table,\n",
    "        classifier_table\n",
    "    )\n",
    "    \n",
    "    # CREATE VALIDATION DATASET WITH REGRESSION ENSEMBLE PREDICTIONS\n",
    "    validation_dataset = regression_ensemble.predict(primary_dataset['test'])\n",
    "    \n",
    "    # CLONE DATASET & ADD LABELS\n",
    "    validation_with_labels = validation_dataset.copy()\n",
    "    validation_with_labels['label'] = primary_dataset['test']['labels'][-len(validation_dataset):]\n",
    "    \n",
    "    # PREDICT WITH THE CLASSIFIER ENSEMBLE\n",
    "    classifier_predictions = classifier_ensemble.predict({\n",
    "        'features': validation_dataset.to_numpy(),\n",
    "        'labels': []\n",
    "    })\n",
    "    \n",
    "    # PUT TRUE LABELS THROUGH DECISION MACHINE\n",
    "    matrix_labels = decision_machine.convert(validation_with_labels)\n",
    "    \n",
    "    # CREATE A CONFUSION MATRIX FOR VALIDATION PREDICTIONS\n",
    "    classifier_matrixes = misc.validation_matrixes(\n",
    "        classifier_matrixes,\n",
    "        classifier_predictions,\n",
    "        matrix_labels\n",
    "    )\n",
    "    \n",
    "    # STITCH TOGETHER REGRESSION FITTING METRICS\n",
    "    regression_fitting = misc.regression_fitting_metrics(regression_ensemble)\n",
    "        \n",
    "    # SAVE EVERYTHING\n",
    "    pipeline_name = storage.save_pipeline({\n",
    "        'config': config,\n",
    "        'regression_ensemble': regression_ensemble,\n",
    "        'classifier_ensemble': classifier_ensemble,\n",
    "        'predictions': {\n",
    "            'regression': {\n",
    "                'training': {\n",
    "                    'graph': 'line',\n",
    "                    'data': json.loads(regression_table.to_json())\n",
    "                },\n",
    "                'validation': {\n",
    "                    'graph': 'line',\n",
    "                    'data': json.loads(validation_with_labels.to_json())\n",
    "                }\n",
    "            },\n",
    "            'classifiers': classifier_matrixes\n",
    "        },\n",
    "        'regression_fitting': regression_fitting\n",
    "    })\n",
    "\n",
    "    return pipeline_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD & PREDICT WITH PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pipeline(name, config):\n",
    "    \n",
    "    # SERIALIZE THE REGRESSION & CLASSIFIER ENSEMBLE\n",
    "    regression_ensemble, classifier_ensemble, pipeline_config = storage.load_pipeline(name)\n",
    "    \n",
    "    # REPLACE OLD DATASET\n",
    "    pipeline_config['data'] = config['data'] # 'C://Users/35840/desktop/coding/python/pipeline/extra/fresh.csv'\n",
    "    \n",
    "    # ADD FEATURES & CREATE REGRESSION DATASET\n",
    "    regression_dataset = create_regression_dataset(pipeline_config)\n",
    "    \n",
    "    # DROP LABEL COLUMN & POP CLOSE COLUMN\n",
    "    regression_dataset.drop(columns=['label'], inplace=True)\n",
    "    closing_prices = regression_dataset.pop('close')\n",
    "    \n",
    "    # PREDICT WITH REGRESSION ENSEMBLE\n",
    "    regression_predictions = regression_ensemble.predict({\n",
    "        'features': regression_dataset.to_numpy(),\n",
    "        'labels': [0] * len(regression_dataset)\n",
    "    })\n",
    "    \n",
    "    # PREDICT WITH CLASSIFIER ENSEMBLE\n",
    "    classifier_predictions = classifier_ensemble.predict({\n",
    "        'features': regression_predictions.to_numpy(),\n",
    "        'labels': []\n",
    "    })\n",
    "    \n",
    "    # ATTACH CLOSING PRICE TO PREDICTIONS\n",
    "    classifier_predictions['close'] = closing_prices.to_numpy()[-len(classifier_predictions):]\n",
    "    \n",
    "    return classifier_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING CREATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_config = storage.load_yaml('configs/train_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression_dataset = create_regression_dataset(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create_pipeline(regression_dataset, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_config = storage.load_yaml('configs/pred_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foob = use_pipeline('PIPELINE-1603537949', pred_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foob.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prof = profit.weighted_position_investing(foob, 100, 0.4, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prof.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#prof['randforest_0'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prof['svc_2'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
