{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYPE-1 FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_k(dataframe, timeframe):\n",
    "    return ((dataframe['close'] - dataframe['low'].rolling(timeframe).min()) / (dataframe['high'].rolling(timeframe).max() - dataframe['low'].rolling(timeframe).min())) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_d(dataframe, timeframe):\n",
    "    return dataframe['sk'].rolling(timeframe).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(dataframe, timeframe):\n",
    "    return dataframe['close'].diff(periods=timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET SPLITTING & NORMALIZING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_split(dataframe, settings):\n",
    "    \n",
    "    # CONVERT DF TO NUMPY ARRAY\n",
    "    rows = dataframe.to_numpy()\n",
    "    \n",
    "    # TRAIN SET INDEX LIMIT\n",
    "    limit = math.ceil(len(rows) * settings['split']['train'])\n",
    "    \n",
    "    # INSTANTIATE NORMALIZER\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # FIT NORMALIZER WITH TRAIN DATA, THEN TRANSFORM TEST DATA\n",
    "    rows[:limit] = scaler.fit_transform(rows[:limit])\n",
    "    rows[limit:] = scaler.transform(rows[limit:])\n",
    "    \n",
    "    # CONTAINERS\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # LOOP THROUGH\n",
    "    for row in rows:\n",
    "        features.append(row[:-1])\n",
    "        labels.append(row[-1])\n",
    "\n",
    "    # RETURN AS DICT\n",
    "    return {\n",
    "        'train': {\n",
    "            'features': np.array(features[:limit]),\n",
    "            'labels': np.array(labels[:limit])\n",
    "        },\n",
    "        'test': {\n",
    "            'features': np.array(features[limit:]),\n",
    "            'labels': np.array(labels[limit:])\n",
    "        },\n",
    "        'scaler': scaler\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_split(dataframe, settings):\n",
    "    \n",
    "    # CONVERT DF TO NUMPY ARRAY\n",
    "    rows = dataframe.to_numpy()\n",
    "    \n",
    "    # TRAIN & VALIDATION SET INDEX LIMITS - 60/20/20 SPLIT\n",
    "    train_limit = math.ceil(len(rows) * 0.6)\n",
    "    validation_limit = math.ceil(len(rows) * 0.8)\n",
    "    \n",
    "    # INSTANTIATE NORMALIZER\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # FIT NORMALIZER WITH TRAIN DATA, THEN TRANSFORM VALIDATION & TEST DATA\n",
    "    rows[:train_limit] = scaler.fit_transform(rows[:train_limit])\n",
    "    rows[train_limit:] = scaler.transform(rows[train_limit:])\n",
    "    \n",
    "    # CONTAINERS\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # LOOP THROUGH\n",
    "    for row in rows:\n",
    "        features.append(row[:-1])\n",
    "        labels.append(row[-1])\n",
    "\n",
    "    # RETURN AS DICT\n",
    "    return {\n",
    "        'train': {\n",
    "            'features': np.array(features[:train_limit]),\n",
    "            'labels': np.array(labels[:train_limit])\n",
    "        },\n",
    "        'validation': {\n",
    "            'features': np.array(features[train_limit:validation_limit]),\n",
    "            'labels': np.array(labels[train_limit:validation_limit])\n",
    "        },\n",
    "        'test': {\n",
    "            'features': np.array(features[validation_limit:]),\n",
    "            'labels': np.array(labels[validation_limit:])\n",
    "        },\n",
    "        'scaler': scaler\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESHAPE FEATURES TO BE THREE DIMENSIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(dataset):\n",
    "    \n",
    "    # RESHAPE TRAIN & TEST FEATURES\n",
    "    dataset['train']['features'] = np.reshape(dataset['train']['features'], (dataset['train']['features'].shape[0], dataset['train']['features'].shape[1], 1))\n",
    "    dataset['validation']['features'] = np.reshape(dataset['validation']['features'], (dataset['test']['features'].shape[0], dataset['validation']['features'].shape[1], 1))\n",
    "    dataset['test']['features'] = np.reshape(dataset['test']['features'], (dataset['test']['features'].shape[0], dataset['test']['features'].shape[1], 1))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD FEATUERS TO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(dataframe, config):\n",
    "    \n",
    "    # DECONSTRUCT RELEVANT PARAMS\n",
    "    window = config['window']\n",
    "    features = config['add']\n",
    "    post_filter = config['filter']\n",
    "    \n",
    "    # AVAILABLE FEATURES\n",
    "    available = {\n",
    "        'sk': stochastic_k,\n",
    "        'sd': stochastic_d,\n",
    "        'momentum': momentum\n",
    "    }\n",
    "    \n",
    "    # ADD EACH FEATURE AS A COLUMN\n",
    "    for name in features:\n",
    "        dataframe[name] = available[name](dataframe, window)\n",
    "    \n",
    "    # FILTER OUT GARBAGE & RETURN\n",
    "    return dataframe.dropna().filter(post_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZE & SPLIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset, name, settings):\n",
    "    \n",
    "    # REGRESSION\n",
    "    if (name == 'linreg'):\n",
    "        return reg_split(dataset, settings)\n",
    "    \n",
    "    # LSTM\n",
    "    elif (name == 'lstm'):\n",
    "        data = lstm_split(dataset, settings)\n",
    "        return reshape_input(data)\n",
    "    \n",
    "    # OTHERWISE, WRITE ERROR\n",
    "    else:\n",
    "        print('BAD SPLITTER TYPE')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
