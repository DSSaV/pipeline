{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general(dataframe, percent):\n",
    "    \n",
    "    # SPLIT DATAFRAME INTO FEATURES & LABELS\n",
    "    features = dataframe.loc[:, dataframe.columns != 'label'].to_numpy()\n",
    "    labels = dataframe[['label']].to_numpy()\n",
    "    \n",
    "    # LENGTH TO SPLIT AT\n",
    "    limit = floor(len(features) * percent)\n",
    "    \n",
    "    # TRAIN/TEST FEATURES\n",
    "    train = features[:limit]\n",
    "    test = features[limit:]\n",
    "    \n",
    "    # SCALED FEATURES\n",
    "    scaled_train, scaled_test, scaler = normalize(train, test)\n",
    "    \n",
    "    return {\n",
    "        'train': {\n",
    "            'features': scaled_train,\n",
    "            'labels': np.ndarray.flatten(labels[:limit])\n",
    "        },\n",
    "        'test': {\n",
    "            'features': scaled_test,\n",
    "            'labels': np.ndarray.flatten(labels[limit:])\n",
    "        }\n",
    "    }, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(temp_train, temp_test):\n",
    "    \n",
    "    # SCALE EVERYTHING FROM 0 TO 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # FIT ON TRAIN DATA, THEN TRANSFORM TEST DATA\n",
    "    train = scaler.fit_transform(temp_train)\n",
    "    test = scaler.transform(temp_test)\n",
    "    \n",
    "    return train, test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(dataset, folds, window=0):\n",
    "    \n",
    "    # DECONSTRUCT PARAMS\n",
    "    features = dataset['features']\n",
    "    labels = dataset['labels']\n",
    "    \n",
    "    # INITIALIZE A X FOLD SPLIT\n",
    "    tss = TimeSeriesSplit(n_splits=folds)\n",
    "    data = []\n",
    "    \n",
    "    # SPLIT DATASET X TIMES\n",
    "    for train_index, test_index in tss.split(features):\n",
    "        \n",
    "        # SHIFT THE VALIDATION INDEX WHEN A SLIDING WINDOW IS GIVEN\n",
    "        if window:\n",
    "            test_index = np.concatenate((train_index[-window:], test_index))\n",
    "        \n",
    "        # APPEND TO CONTAINER\n",
    "        data.append({\n",
    "            'train': {\n",
    "                'features': features[train_index],\n",
    "                'labels': labels[train_index]\n",
    "            },\n",
    "            'test': {\n",
    "                'features': features[test_index],\n",
    "                'labels': labels[test_index]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataset, params, shuffle=True):\n",
    "    \n",
    "    # DECONSTRUCT DATASET\n",
    "    features = dataset['features']\n",
    "    labels = dataset['labels']\n",
    "    \n",
    "    # DECONSTRUCT PARAMS\n",
    "    batch = params['batch']\n",
    "    window = params['window']\n",
    "    \n",
    "    # GENERATE & RETURN\n",
    "    return TimeseriesGenerator(\n",
    "        features,\n",
    "        labels,\n",
    "        length=window,\n",
    "        batch_size=batch,\n",
    "        shuffle=shuffle\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(dataset):\n",
    "    \n",
    "    # COMBINE SCALED FEATURES & LABELS FOR GRID SEARCHING\n",
    "    features = np.concatenate((dataset['train']['features'], dataset['test']['features']))\n",
    "    labels = np.concatenate((dataset['train']['labels'], dataset['test']['labels']))\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
